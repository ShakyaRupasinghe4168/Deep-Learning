{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "# Transformers with Keras\n",
    "\n",
    "Estimated time needed **45** mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to use the Keras library to build a transformer using a sequence-to-sequence architecture with self-attention for translation. We will train the model using a sample dataset and then use this model for English to Spanish translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives for this Notebook    \n",
    "* How to use the Keras library to build transformers model\n",
    "* Train the transformer model using a given dataset\n",
    "* Use the trained transformer model to translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 4>\n",
    "1. <a href=\"#Import-Keras-and-Packages\">Import Keras and Packages</a><br>\n",
    "2. <a href=\"#Step-1:-Data-Preparation\">Step 1: Data Preparation</a><br>\n",
    "3. <a href=\"#Step-2:-Self-Attention-Layer\">Step 2: Self-Attention Layer</a><br>\n",
    "4. <a href=\"#Step-3:-Model-Architecture\">Step 3: Model Architecture</a><br>\n",
    "5. <a href=\"#Step-4:-Training-the-Model\">Step 4: Training the Model</a><br>\n",
    "6. <a href=\"#Step-5:-Plotting-the-training-loss\">Step 5: Plotting the training loss</a><br>\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.1\n",
      "  Downloading tensorflow-2.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.17.1)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.76.0)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow==2.17.1)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.11.3)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow==2.17.1)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.17.1) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.1) (0.1.2)\n",
      "Downloading tensorflow-2.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.4/601.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: protobuf, numpy, tensorboard, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cpu 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 protobuf-4.25.8 tensorboard-2.17.1 tensorflow-2.17.1\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "==== All required libraries are installed =====\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.1\n",
    "!pip install matplotlib==3.9.2\n",
    "\n",
    "print(\"==== All required libraries are installed =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress the tensorflow warning messages\n",
    "We use the following code to  suppress the warning messages due to use of CPU architechture for tensoflow.\n",
    "\n",
    "You may want to **comment out** these lines if you are using the GPU architechture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To use Keras, you will also need to install a backend framework – such as TensorFlow.\n",
    "\n",
    "If you install TensorFlow 2.16 or above, it will install Keras by default.\n",
    "\n",
    "We are using the CPU version of tensorflow since we are dealing with smaller datasets. \n",
    "You may install the GPU version of tensorflow on your machine to accelarate the processing of larger datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 19:52:01.868358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-25 19:52:01.924848: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-25 19:52:01.966676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "We start by define the sentences and text for translation training\n",
    "Sentence Pairs: Defines a small dataset of English-Spanish sentence pairs.\n",
    "Target Sequences:\n",
    "Prepends \"startseq\" and appends \"endseq\" to each target sentence for the decoder to learn when to start and stop translating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parallel sentences (English -> Spanish)\n",
    "input_texts = [\n",
    "    \"Hello.\", \"How are you?\", \"I am learning machine translation.\", \"What is your name?\", \"I love programming.\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hola.\", \"¿Cómo estás?\", \"Estoy aprendiendo traducción automática.\", \"¿Cuál es tu nombre?\", \"Me encanta programar.\"\n",
    "]\n",
    "\n",
    "target_texts = [\"startseq \" + x + \" endseq\" for x in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we convert the text from the sentences to tokens and create a vocabulary\n",
    "Tokenization: Uses Tokenizer to convert words into numerical sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pad the corresponding sentences\n",
    "Padding: Ensures all sequences have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "max_input_length = max([len(seq) for seq in input_sequences])\n",
    "max_output_length = max([len(seq) for seq in output_sequences])\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target data for training\n",
    "decoder_input_data = output_sequences[:, :-1]\n",
    "decoder_output_data = output_sequences[:, 1:]\n",
    "\n",
    "# Convert to one-hot\n",
    "decoder_output_data = np.array([np.eye(output_vocab_size)[seq] for seq in decoder_output_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Self-Attention Layer\n",
    "Self-attention is a mechanism that allows a model to **focus on relevant parts of the input sequence** while processing each word. This is particularly useful in:\n",
    "1) Machine Translation (e.g., aligning words correctly)\n",
    "2) Text Summarization\n",
    "3) Speech Recognition\n",
    "4) Image Processing (Vision Transformers)\n",
    "In this implementation, self-attention is used for text based sequence-to-sequence modeling.\n",
    "\n",
    "\n",
    "Self-Attention works for a given an input sequence by computing a weighted representation of all words for each position. It does so using three key components:\n",
    "\n",
    "1. Query **(Q)**, Key **(K)**, and Value **(V)** Matrices\n",
    "For each word (token) in a sequence:\n",
    "\n",
    "Query (Q): What this word is looking for.\n",
    "Key (K): What this word represents.\n",
    "Value (V): The actual information in the word.\n",
    "\n",
    "2. Compute **Attention Scores**\n",
    "Next, we **calculate the similarity between each query and key** using dot-product attention:\n",
    "Each word in a sequence attends to every other word based on these scores.\n",
    "\n",
    "3. Apply **Scaling & Softmax**\n",
    "Since dot-product values can be large, we scale them. \n",
    "Next, Applying softmax converts scores into attention weights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention class\n",
    "In this implementation of self-attention layer:\n",
    "1. We first initialize the weights in the **build** method, where:\n",
    "    1. **self.Wq**, **self.Wk**, **self.Wv** are the trainable weight matrices.\n",
    "    2. Their **shape is (feature_dim, feature_dim)**, meaning they transform input features into Q, K, and V representations.\n",
    "2. Applying Attention using **call** method. The **call()** method:\n",
    "   1. Computes **Q, K, V** by multiplying inputs (encoder/decoder output) with their respective weight matrices.\n",
    "   2. Computes **dot-product attention scores** using K.batch_dot(q, k, axes=[2, 2]), resulting in a (batch_size, seq_len, seq_len) matrix.\n",
    "   3. **Scales** the scores to avoid large values.\n",
    "   4. Applies **softmax** to normalize the attention scores.\n",
    "   5. **Multiplies attention weights with V** to get the final output.\n",
    "3. The **compute_output_shape** method defines the shape of the output tensor after the layer processes an input.\n",
    "    1. The output shape of the Self-Attention layer **remains the same** as the input shape.\n",
    "    2. The attention mechanism **transforms** the input but does not change its dimensions.4\n",
    "    3. If the attention layer changed the shape, you would modify compute_output_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Architecture\n",
    "The model follows an Encoder-Decoder structure:\n",
    "\n",
    "### Encoder:\n",
    "1) Takes input sentences (padded and tokenized).\n",
    "2) Uses an Embedding layer (word representations) + LSTM (to process sequences).\n",
    "    1. The LSTMs are used as the **help process variable-length input sentences** and generate meaningful translations.\n",
    "4) Outputs context vectors (hidden & cell states).\n",
    "\n",
    "### Attention Layer\n",
    "1) Applied to both the encoder and decoder outputs.\n",
    "2) Helps the decoder focus on relevant words during translation.\n",
    "\n",
    "### Decoder\n",
    "1) Receives target sequences (shifted one step ahead).\n",
    "2) Uses an LSTM with encoder states as initial states.\n",
    "3) Applies self-attention for better learning.\n",
    "4) Uses a Dense layer (Softmax) to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>) │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,721</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,096\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,352\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mAdditiveAttention\u001b[0m) │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m17\u001b[0m)     │      \u001b[38;5;34m8,721\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,049</span> (4.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,068,049\u001b[0m (4.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,049</span> (4.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,068,049\u001b[0m (4.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, Dense, Embedding, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    " \n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    " \n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    " \n",
    "# Attention: decoder attends to encoder outputs\n",
    "attention = AdditiveAttention()\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    " \n",
    "# Combine decoder outputs with attention context\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    " \n",
    "# Final Dense layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    " \n",
    "# Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training the Model\n",
    "Uses categorical_crossentropy as the loss function since output words are one-hot encoded.\n",
    "Trains using Adam optimizer for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - accuracy: 0.0400 - loss: 2.8310\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.2800 - loss: 2.7977\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.3200 - loss: 2.7623\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 0.3200 - loss: 2.7213\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.3200 - loss: 2.6708\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.3200 - loss: 2.6058\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.2800 - loss: 2.5203\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.2800 - loss: 2.4080\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.2400 - loss: 2.2703\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - accuracy: 0.2400 - loss: 2.1349\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.2400 - loss: 2.0619\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.2400 - loss: 2.0531\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.2400 - loss: 2.0164\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - accuracy: 0.2800 - loss: 1.9426\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.3600 - loss: 1.8775\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.4400 - loss: 1.8407\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.4400 - loss: 1.7971\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - accuracy: 0.4400 - loss: 1.7377\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 0.4800 - loss: 1.6743\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - accuracy: 0.5600 - loss: 1.6149\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 0.5600 - loss: 1.5600\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.4800 - loss: 1.5073\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 0.4800 - loss: 1.4540\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 0.4800 - loss: 1.3967\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.4800 - loss: 1.3323\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step - accuracy: 0.5200 - loss: 1.2592\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 0.6000 - loss: 1.1791\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - accuracy: 0.6400 - loss: 1.0976\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.6800 - loss: 1.0208\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.7200 - loss: 0.9519\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.7200 - loss: 0.8903\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - accuracy: 0.7200 - loss: 0.8336\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 0.8000 - loss: 0.7795\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - accuracy: 0.8400 - loss: 0.7250\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 0.8400 - loss: 0.6698\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.8800 - loss: 0.6166\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.9600 - loss: 0.5674\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761ms/step - accuracy: 1.0000 - loss: 0.5213\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.4790\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - accuracy: 1.0000 - loss: 0.4398\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step - accuracy: 1.0000 - loss: 0.4010\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 1.0000 - loss: 0.3649\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.3337\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - accuracy: 1.0000 - loss: 0.3056\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460ms/step - accuracy: 1.0000 - loss: 0.2797\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 1.0000 - loss: 0.2552\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 1.0000 - loss: 0.2334\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 1.0000 - loss: 0.2134\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 1.0000 - loss: 0.1951\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 1.0000 - loss: 0.1786\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 0.1635\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 1.0000 - loss: 0.1487\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step - accuracy: 1.0000 - loss: 0.1358\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 1.0000 - loss: 0.1244\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 1.0000 - loss: 0.1141\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 0.1043\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 1.0000 - loss: 0.0957\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 1.0000 - loss: 0.0879\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 1.0000 - loss: 0.0807\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - accuracy: 1.0000 - loss: 0.0744\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 1.0000 - loss: 0.0686\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 1.0000 - loss: 0.0630\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step - accuracy: 1.0000 - loss: 0.0580\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 1.0000 - loss: 0.0536\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 1.0000 - loss: 0.0496\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - accuracy: 1.0000 - loss: 0.0459\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 1.0000 - loss: 0.0425\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - accuracy: 1.0000 - loss: 0.0394\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step - accuracy: 1.0000 - loss: 0.0366\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 557ms/step - accuracy: 1.0000 - loss: 0.0341\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 1.0000 - loss: 0.0318\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0298\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step - accuracy: 1.0000 - loss: 0.0279\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - accuracy: 1.0000 - loss: 0.0262\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 1.0000 - loss: 0.0246\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 1.0000 - loss: 0.0232\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 1.0000 - loss: 0.0219\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 0.0207\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 1.0000 - loss: 0.0196\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - accuracy: 1.0000 - loss: 0.0186\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.0176\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 0.0168\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 1.0000 - loss: 0.0160\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 1.0000 - loss: 0.0153\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - accuracy: 1.0000 - loss: 0.0146\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 1.0000 - loss: 0.0139\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0134\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 1.0000 - loss: 0.0128\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 0.0123\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 1.0000 - loss: 0.0118\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step - accuracy: 1.0000 - loss: 0.0114\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.0110\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - accuracy: 1.0000 - loss: 0.0106\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 1.0000 - loss: 0.0102\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 519ms/step - accuracy: 1.0000 - loss: 0.0095\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 1.0000 - loss: 0.0092\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 1.0000 - loss: 0.0090\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 1.0000 - loss: 0.0087\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 1.0000 - loss: 0.0084\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "history_glorot_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plotting the training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome, now you have succesfully trained a transformers model.\n",
    "### Now let's try some practice excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, let's train the model using \"he_uniform\" initializer instead of \"glorot_uniform\". Then, compare the training loss between model using \"glorot_uniform\" vs \"he_uniform\" initializers by plotting them using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - accuracy: 0.0400 - loss: 2.8343\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.4000 - loss: 2.8026\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step - accuracy: 0.3600 - loss: 2.7692\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.3200 - loss: 2.7309\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 0.3200 - loss: 2.6842\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - accuracy: 0.2800 - loss: 2.6246\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step - accuracy: 0.2800 - loss: 2.5463\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - accuracy: 0.2800 - loss: 2.4427\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376ms/step - accuracy: 0.2800 - loss: 2.3113\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - accuracy: 0.2800 - loss: 2.1702\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - accuracy: 0.2800 - loss: 2.0801\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 0.2800 - loss: 2.0751\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step - accuracy: 0.2800 - loss: 2.0528\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - accuracy: 0.3200 - loss: 1.9776\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 0.3600 - loss: 1.8910\n",
      "Epoch 16/100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "\n",
    "#Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, try to use adaptive gradient optimizer instead of adam. Then, plot and compare the results between adam and adaptive gradient optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n",
    "plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n",
    "plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2024-11-20  | 1.0  | Aman  |  Created the lab |\n",
    "<hr>\n",
    "-->\n",
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "3a07fb4049049613c9f3bf3a0aaeeac466433593dd808e2778bab531403fe8a9"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
